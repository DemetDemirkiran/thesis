root_path: "/home/demet/Desktop/Chest_Dataset/Chest_Dataset_Resize"

## Chest14: /home/demet/Desktop/Chest_Dataset/Chest_Dataset_Resize
## CheXpert: /home/demet/Desktop/CheXpert-v1.0-small

mode: "test"

data:
  size: [224, 224]

model:
  model_type: "cbam"
  number_classes: 15

## chex_small - does a smaller amount because teh dataset was too big for testing
## chest14 - does entire dataset
## chex - when you want to do the entire dataset

train:
  name: "chest14"
  data_path: "/home/demet/Desktop/CheXpert-v1.0-small"
  start_epoch: 0
  end_epoch: 6
  batch: 16
  num_workers: 0
  chkpnt_step: 1
  loss: "focal"
  metric: "triplet"

## loss: bce , cel , wcel , focal
## metric: contrastive , triplet , proxy or blank for none


  learning_rate:
    momentum: 0.9
    decay: 0.1
    base_rate: 0.0001
    steps: 3

## Chest14: /home/demet/Desktop/Chest_Dataset/Chest_Dataset_Resize
## CheXpert: /home/demet/Desktop/CheXpert-v1.0-small
## chex_small
## chest14

test:
  name: "chex"
  dataset_path: "/home/demet/Desktop/CheXpert-v1.0-small"
  checkpoint_path: "/home/demet/PycharmProjects/thesis/training_logs/cbam/focal/chex/focal_contrastive_cbam_lr0.0001_bs16/5.pth"
  batch: 128
  num_workers: 0


experiment_name: "focal_triplet_"
log_dir: "/home/demet/PycharmProjects/thesis/training_logs/cbam/focal"



  # batchsize = [64]  # [64, 128, 256, 512]
  # learningrate = [1e-3, 1e-4, 1e-5]
